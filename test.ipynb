{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "\n",
    "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"who is the prime minister of india\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "youtube_url = \"https://youtu.be/zIwLWfaAg-8?si=qsN6eA17rsIqJl8o\"\n",
    "loader = YoutubeLoader.from_youtube_url(youtube_url=youtube_url, add_video_info=False)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = f\"\"\"\n",
    "You are a helpful assistant programmed to generate questions based on any text provided. For every chunk of text you receive, you're tasked with designing 10 distinct questions. Each of these questions will be accompanied by 3 possible answers: one correct answer and two incorrect ones. \n",
    "provided text is as follows:\n",
    "{docs}\n",
    "\n",
    "For clarity and ease of processing, structure your response in a way that emulates a Python list of lists. \n",
    "\n",
    "Your output should be shaped as follows:\n",
    "\n",
    "1. An outer list that contains 5 inner lists.\n",
    "2. Each inner list represents a set of question and answers, and contains exactly 4 strings in this order:\n",
    "- The generated question.\n",
    "- The correct answer.\n",
    "- The first incorrect answer.\n",
    "- The second incorrect answer.\n",
    "\n",
    "Your output should mirror this structure:\n",
    "[\n",
    "    [\"Generated Question 1\", \"Correct Answer 1\", \"Incorrect Answer 1.1\", \"Incorrect Answer 1.2\"],\n",
    "    [\"Generated Question 2\", \"Correct Answer 2\", \"Incorrect Answer 2.1\", \"Incorrect Answer 2.2\"],\n",
    "    ...\n",
    "]\n",
    "\n",
    "It is crucial that you adhere to this format as it's optimized for further Python processing.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "a = llm.invoke(template).content\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Provide your Google API Key\")\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "que_no = 10\n",
    "\n",
    "q = f\"\"\"\n",
    "You are a helpful assistant programmed to generate questions based on any text provided. For every chunk of text you receive, you're tasked with designing {que_no} distinct questions. Each of these questions will be accompanied by 3 possible answers: one correct answer and two incorrect ones. \n",
    "provided text is deliminated in tripple ticks:\n",
    "```{docs}```\n",
    "\n",
    "For clarity and ease of processing, structure your response in a way that emulates a Python list of lists. \n",
    "\n",
    "Your output should be shaped as follows:\n",
    "\n",
    "1. An outer list that contains {que_no} inner lists.\n",
    "2. Each inner list represents a set of question and answers, and contains exactly 4 strings in this order:\n",
    "- The generated question.\n",
    "- The correct answer.\n",
    "- The first incorrect answer.\n",
    "- The second incorrect answer.\n",
    "\n",
    "Your output should mirror this structure:\n",
    "[\n",
    "    [\"Generated Question 1\", \"Correct Answer 1\", \"Incorrect Answer 1.1\", \"Incorrect Answer 1.2\"],\n",
    "    [\"Generated Question 2\", \"Correct Answer 2\", \"Incorrect Answer 2.1\", \"Incorrect Answer 2.2\"],\n",
    "    ...\n",
    "]\n",
    "\n",
    "It is crucial that you adhere to this format as it's optimized for further Python processing.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "otpt = chain.invoke({\"query\":q})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "output = [['Which model is considered the grandfather of all life cycle models?',\n",
    "  'Waterfall model',\n",
    "  'Agile model',\n",
    "  'Scrum model'],\n",
    " ['In the waterfall model, what is the final phase?',\n",
    "  'System testing',\n",
    "  'Requirements gathering',\n",
    "  'Design'],\n",
    " ['What is an advantage of the waterfall model?',\n",
    "  'Finding errors early',\n",
    "  'Flexibility',\n",
    "  'Rapid development'],\n",
    " ['What is a disadvantage of the waterfall model?',\n",
    "  'Lack of flexibility',\n",
    "  'High cost',\n",
    "  'Long development time'],\n",
    " ['When is the waterfall model most suitable?',\n",
    "  'Stable product definition and well-known domain',\n",
    "  'Changing requirements and evolving technology',\n",
    "  'Small and simple projects']]\n",
    "\n",
    "\n",
    "def get_randomized_options(options):\n",
    "    correct_answer = options[0]\n",
    "    random.shuffle(options)\n",
    "    return options, correct_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import streamlit as st\n",
    "import os\n",
    "import json\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "def get_randomized_options(options):\n",
    "    correct_answer = options[0]\n",
    "    random.shuffle(options)\n",
    "    return options, correct_answer\n",
    "\n",
    "st.title(\"üìöüß†Youtube Video Quiz App\")\n",
    "\n",
    "\n",
    "name = st.text_input(\"üë©‚Äçüíº Enter your name: \")\n",
    "url = st.text_input(\"üîó Enter your video url: \")\n",
    "process =  st.checkbox(\"Submit and Process\")\n",
    "\n",
    "if process:\n",
    "    # Initialize variables\n",
    "    st.write(f\"Hello, {name}!!\")\n",
    "    # youtube_url = \"https://youtu.be/zIwLWfaAg-8?si=qsN6eA17rsIqJl8o\"\n",
    "\n",
    "    with st.spinner(\"Loading youtube content...\"):\n",
    "        loader = YoutubeLoader.from_youtube_url(youtube_url=url, add_video_info=False)\n",
    "        doc = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 1500,\n",
    "        chunk_overlap = 150\n",
    "    )\n",
    "    docs = text_splitter.split_documents(doc)\n",
    "\n",
    "\n",
    "    que_no = 8\n",
    "\n",
    "    q = f\"\"\"\n",
    "    You are a helpful assistant programmed to generate questions based on any text provided. For every chunk of text you receive, you're tasked with designing {que_no} distinct questions. Each of these questions will be accompanied by 3 possible answers: one correct answer and two incorrect ones. \n",
    "    provided text is deliminated in tripple ticks:\n",
    "    ```{docs}```\n",
    "\n",
    "    For clarity and ease of processing, structure your response in a way that emulates a Python list of lists. \n",
    "\n",
    "    Your output should be shaped as follows:\n",
    "\n",
    "    1. An outer list that contains {que_no} inner lists.\n",
    "    2. Each inner list represents a set of question and answers, and contains exactly 4 strings in this order: (it should STRICTLY contain 4 strings)\n",
    "    - The generated question.\n",
    "    - The correct answer.\n",
    "    - The first incorrect answer.\n",
    "    - The second incorrect answer.\n",
    "\n",
    "    Your output should mirror this structure:\n",
    "    [\n",
    "        [\"Generated Question 1\", \"Correct Answer 1\", \"Incorrect Answer 1.1\", \"Incorrect Answer 1.2\"],\n",
    "        [\"Generated Question 2\", \"Correct Answer 2\", \"Incorrect Answer 2.1\", \"Incorrect Answer 2.2\"],\n",
    "        ...\n",
    "    ]\n",
    "\n",
    "    It is crucial that you adhere to this format as it's optimized for further Python processing.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    parser = JsonOutputParser()\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "        input_variables=[\"query\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | parser\n",
    "\n",
    "    output = chain.invoke({\"query\":q})\n",
    "\n",
    "\n",
    "    score = 0\n",
    "    total_questions = len(output)\n",
    "    incorrect_answers = []\n",
    "\n",
    "\n",
    "    with st.form(\"quiz_form\"):\n",
    "        for i, question_data in enumerate(output, start=1):\n",
    "            question, correct_answer, incorrect_answer1, incorrect_answer2 = question_data\n",
    "            \n",
    "            # Randomize options\n",
    "            options, correct_answer = get_randomized_options([correct_answer, incorrect_answer1, incorrect_answer2])\n",
    "            \n",
    "            st.write(f\"{i}. {question}\")\n",
    "\n",
    "            selected_option = st.radio(f\"Choose the correct answer for Question {i}:\", options, key=i)\n",
    "            \n",
    "            st.session_state[f\"selected_option_{i}\"] = selected_option\n",
    "\n",
    "        submit_button = st.form_submit_button(\"Submit\")\n",
    "    \n",
    "\n",
    "    if submit_button:\n",
    "        # Iterate through each question to display the correct answer and update score\n",
    "        for i, question_data in enumerate(output, start=1):\n",
    "            correct_answer = question_data[1]\n",
    "            selected_option = st.session_state[f\"selected_option_{i}\"]\n",
    "\n",
    "            if selected_option == correct_answer:\n",
    "                score += 1\n",
    "            else:\n",
    "                incorrect_answers.append(i)\n",
    "        \n",
    "\n",
    "        st.header(\"üéâQuiz Summary\")\n",
    "\n",
    "        for i in range(total_questions):\n",
    "            if i+1 in incorrect_answers:\n",
    "                # st.write(f\"‚ùåQue {i+1}: incorrect\")\n",
    "                st.write(f\"‚ùå Question {i+1}: {output[i][0]}\")\n",
    "                st.write(f\"Correct ans: {output[i][1]}\")\n",
    "\n",
    "            else:\n",
    "                # st.write(f\"‚úîÔ∏èQue {i+1}: correct\")\n",
    "                st.write(f\"‚úîÔ∏è Question {i+1}: {output[i][0]}\")\n",
    "                # st.write(f\"Correct ans: {output[i][1]}\")\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # Save the user information and quiz summary to a JSON file\n",
    "        quiz_data = {\n",
    "                    \"name\": name,\n",
    "                    \"questions\": output,\n",
    "                    \"score\": score\n",
    "                }\n",
    "\n",
    "        # Define the output file name\n",
    "        output_file_name = f\"result.json\"\n",
    "\n",
    "        # Check if the output file already exists\n",
    "        if os.path.exists(output_file_name):\n",
    "            # Load existing data from the file\n",
    "            with open(output_file_name, \"r\") as file:\n",
    "                existing_data = json.load(file)\n",
    "            \n",
    "            # Append new data to the existing data\n",
    "            existing_data.append(quiz_data)\n",
    "            \n",
    "            # Write the updated data back to the file\n",
    "            with open(output_file_name, \"w\") as file:\n",
    "                json.dump(existing_data, file)\n",
    "        else:\n",
    "            # Write the JSON data to the file if it doesn't exist\n",
    "            with open(output_file_name, \"w\") as file:\n",
    "                json.dump([quiz_data], file)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
